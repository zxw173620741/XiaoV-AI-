from ast import mod
from pyexpat import model
import stat
from tkinter import W, Listbox
from turtle import st
from typing import Dict, List, TypedDict
from unittest import removeResult
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph, END
from ollama import *
import re
import traceback
import json
import os
import time
from tenacity import retry
from tqdm import tqdm
from dotenv import load_dotenv

load_dotenv()
api_url = os.getenv("OLLAMA_URL")
class AgentState(TypedDict):
    problem:str
    reasons:List[str]
    knowledge:Dict[str,Dict[str,str]]
    model_advice:Dict[str,str]

class VisualLogger:
    @staticmethod
    def step_start(msg):
        print(f"\n\033[95m{'=' * 50}\033[0m")
        print(f"\033[95mğŸš€ {msg}\033[0m")
        print(f"\033[95m{'=' * 50}\033[0m")

    @staticmethod
    def step_info(msg):
        print(f"\033[94mâ„¹ï¸ {msg}\033[0m")

    @staticmethod
    def step_success(msg):
        print(f"\033[92mâœ… {msg}\033[0m")

    @staticmethod
    def step_warning(msg):
        print(f"\033[93mâš ï¸ {msg}\033[0m")

    @staticmethod
    def show_state(state):
        print("\n\033[96må½“å‰çŠ¶æ€:\033[0m")
        for key, value in state.items():
            if key == "knowledge" or key == "model_advice":
                print(f"  \033[93m{key}:\033[0m")
                for subkey, subvalue in value.items():
                    print(f"    \033[93m{subkey}:\033[0m {str(subvalue)[:80]}...")
            else:
                print(f"  \033[93m{key}:\033[0m {value}")

def query_knowledge_base(problem: str, reason: str) -> dict:
    """è°ƒç”¨çŸ¥è¯†åº“è¿›è¡ŒæŸ¥è¯¢"""
    VisualLogger.step_info(f"æŸ¥è¯¢çŸ¥è¯†åº“: é—®é¢˜={problem}, å¼‚å¸¸æŒ‡æ ‡={reason}")

    default_result = {"solution": "æš‚æ— æ–¹æ¡ˆ", "case": "æ— ç›¸å…³æ¡ˆä¾‹"}

    knowledge_file = "knowledge_base/production_knowledge"

    if not os.path.exists(knowledge_file):
        VisualLogger.step_warning(f"çŸ¥è¯†åº“æ–‡ä»¶ {knowledge_file} ä¸å­˜åœ¨")
        return default_result

    try:
        with open(knowledge_file, 'r', encoding='utf-8') as f:
            content = f.read()

        sections = content.strip().split("\n\n")

        for section in sections:
            lines = section.strip().split("\n")
            if not lines:
                continue

            section_reason = lines[0].strip()
            if section_reason != reason:
                continue

            solution_lines = []
            case_lines = []
            current_section = None

            for line in lines[1:]:
                if line.startswith("è§£å†³æ–¹æ¡ˆ:"):
                    current_section = "solution"
                    continue
                elif line.startswith("æ¡ˆä¾‹:"):
                    current_section = "case"
                    continue

                if current_section == "solution":
                    solution_lines.append(line.strip())
                elif current_section == "case":
                    case_lines.append(line.strip())

            result = {
                "solution": "\n".join(solution_lines),
                "case": " ".join(case_lines)
            }

            VisualLogger.step_info(f"çŸ¥è¯†åº“è¿”å›ç»“æœ: {result}")
            return result

        VisualLogger.step_warning(f"æœªæ‰¾åˆ°åŸå›  '{reason}' çš„è§£å†³æ–¹æ¡ˆ")
        return default_result

    except Exception as e:
        VisualLogger.step_warning(f"è¯»å–çŸ¥è¯†åº“æ–‡ä»¶å‡ºé”™: {str(e)}")
        return default_result

def generate_model_advice(problem: str, reason: str, knowledge: dict) -> dict:
    """è°ƒç”¨å¤§æ¨¡å‹åˆ†æé—®é¢˜"""
    VisualLogger.step_info(f"è°ƒç”¨å¤§æ¨¡å‹å¤„ç†: é—®é¢˜={problem}, åŸå› ={reason}")

    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªèµ„æ·±ç”Ÿäº§ç®¡ç†é¡¾é—®ã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ä¸ºé—®é¢˜æä¾›ä¸“ä¸šå»ºè®®ï¼š

    **æ ¸å¿ƒé—®é¢˜**: {problem}
    **å…·ä½“åŸå› **: {reason}

    **çŸ¥è¯†åº“è§£å†³æ–¹æ¡ˆ**:
    {knowledge['solution']}

    **çŸ¥è¯†åº“å…¸å‹æ¡ˆä¾‹**:
    {knowledge['case']}

    è¯·å‚è€ƒæ¡ˆä¾‹ç”Ÿæˆå¯¹è¯¥åŸå› çš„ä¸“ä¸šå»ºè®®,300å­—ä»¥å†…ï¼š
    è¯·ä½¿ç”¨ä»¥ä¸‹æ ¼å¼:
    <å»ºè®®å¼€å§‹>
    ä½ çš„å»ºè®®å†…å®¹...
    <å»ºè®®ç»“æŸ>

    """

    try:
        print("å‘é€æç¤ºè¯ç»™æ¨¡å‹...")
        client = Client(host=api_url)
        response = client.chat(
            model='deepseek-r1:8b',
            messages=[{'role': 'user', 'content': prompt}],
            options={'temperature': 0.3, 'num_ctx': 4096}
        )
        output = response['message']['content']

        advice = ""

        advice_match = re.search(r'<å»ºè®®å¼€å§‹>([\s\S]*?)<å»ºè®®ç»“æŸ>', output)

        if advice_match:
            advice = advice_match.group(1).strip()
        else:
            VisualLogger.step_warning("æœªæ‰¾åˆ°å»ºè®®éƒ¨åˆ†æ ‡è®°")
            if "å»ºè®®" in output or "æ–¹æ¡ˆ" in output:
                advice = output.split("\n\n")[0].strip()
            else:
                advice = "æœªèƒ½æå–å»ºè®®å†…å®¹"

        return {"advice": advice}

    except Exception as e:
        VisualLogger.step_warning(f"æ¨¡å‹è°ƒç”¨å‡ºé”™: {str(e)}")
        traceback.print_exc()
        return {"advice": "æ¨¡å‹è°ƒç”¨å‡ºé”™", "summary": "è¯·æ£€æŸ¥æ¨¡å‹æœåŠ¡"}

def process_reason(state: AgentState, config: RunnableConfig) -> AgentState:
    print(f"\n{'=' * 50}")
    print(f"å¼€å§‹åˆ†æå¼‚å¸¸æŒ‡æ ‡: {state['reasons'][0]}")
    reason=state['reasons'][0]
    knowledge=query_knowledge_base(state['problem'],reason)
    model_output=generate_model_advice(state['problem'],reason,knowledge)
    new_state={
        "problem":state['problem'],
        "reasons":state['reasons'][1:],
        "knowledge":{**state["knowledge"],reason:knowledge},
        "model_advice":{**state['model_advice'],reason:model_output ['advice']}
    }
    print(f"é’ˆå¯¹å¼‚å¸¸æŒ‡æ ‡{reason}åˆ†æå®Œæˆ")
    return new_state

def should_continue(state: AgentState) -> str:
    """åˆ¤æ–­å¼‚å¸¸æŒ‡æ ‡æ˜¯å¦å¤„ç†å®Œæˆ"""
    continue_flag = "continue" if state["reasons"] else "end"
    return continue_flag

def create_workflow():
    workflow=StateGraph(AgentState)
    workflow.add_node("process_reason",process_reason)
    workflow.add_conditional_edges(
        "process_reason",
        should_continue,{
            "continue":"process_reason",
            "end":END
        }
    )
    workflow.set_entry_point("process_reason")
    return workflow.compile()

def visualize_final_result(final_state):
    print("\n\033[1;35m" + "=" * 50)
    print("âœ¨ æœ€ç»ˆé—®é¢˜åˆ†ææŠ¥å‘Š âœ¨")
    print("=" * 50 + "\033[0m")

    print(f"\n\033[1;34mæ ¸å¿ƒé—®é¢˜: {final_state['problem']}\033[0m")

    for reason in final_state['knowledge'].keys():
        print("\n\033[1;32m" + "=" * 30 + f" å¼‚å¸¸æŒ‡æ ‡: {reason} " + "=" * 30 + "\033[0m")

        print("\n\033[93mğŸ“š çŸ¥è¯†åº“è§£å†³æ–¹æ¡ˆ:\033[0m")
        print(final_state['knowledge'][reason]['solution'])
        print("\n\033[96mğŸ’¡ å¤§æ¨¡å‹å»ºè®®:\033[0m")
        print(final_state['model_advice'][reason])
        print("\n\033[95mğŸ” ç›¸å…³æ¡ˆä¾‹å‚è€ƒ:\033[0m")
        print(final_state['knowledge'][reason]['case'])

if __name__ == "__main__":
    VisualLogger.step_start("å¯åŠ¨å·¥ä½œæµ")
    initial_state: AgentState = {
        "problem": "production_efficiency",
        "reasons": ["inventory_turnover_rate", "work_hours", "defect_rate"],
        "knowledge": {},
        "model_advice": {}
    }

    VisualLogger.step_info(f"åˆå§‹é—®é¢˜: {initial_state['problem']}")
    VisualLogger.step_info(f"å¾…åˆ†æå¼‚å¸¸æŒ‡æ ‡: {', '.join(initial_state['reasons'])}")
    workflow = create_workflow()
    VisualLogger.step_info("å·¥ä½œæµåˆ›å»ºå®Œæˆï¼Œå¼€å§‹æ‰§è¡Œ...")
    final_state = workflow.invoke(initial_state)
    visualize_final_result(final_state)
    VisualLogger.step_success("å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼")